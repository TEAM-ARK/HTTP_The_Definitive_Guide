# 9장. 웹 로봇

**웹 로봇이란?**
- 사람과의 상호작용 없이 연속된 웹 트랜잭션들을 자동으로 수행하는 SW 프로그램
- 무수히 많은 웹을 떠돌아다님
- 크롤러, 스파이더, 웜, 봇
  - 한 웹페이지를 가져오고 그 페이지가 가리키는 웹 페이지를 계속 타고들어감

> 책에서는 방식에 따라 다른 이름으로 불린다는데 찾아보니 단순히 웹 크롤러의 다른이름이 스파이더, 웜, 봇이라 한다고 함

<br>

## 9.1 크롤러와 크롤링
웹 페이지들을 재귀적으로 타고 기어다닌다(crawl)는 의미에서 `Crawler`!!

인터넷 검색엔진이 웹의 문서를 끌어와 검색 DB에 저장한 후 사용하기 위해 크롤러를 사용

### 루트 집합
일반적으로 자료구조에서의 `Tree`와 `Graph`를 섞어놓은 형태로 보인다.

좋은 루트 집합을 가지고 있다면 그래프 탐색과 같이 계속 타고 들어가면서 모든 웹을 방문할 수 있다.

### 링크 추출
크롤러가 계속 웹을 기어다니며 새로운 링크, 즉 이전에 탐색하지 않았던 링크들이 계속 추가된다.

### 순환 피하기
무한 참조, 순환 참조는 모든 곳에서 문제가 되는 것 같다.

로봇이 함정에 빠지는 상황을 방지하기 위해 어디를 방문했는지를 알고 있어야한다.

로봇도 누군가의 필요로 위해 동작하는 리소스를 사용하는 프로그램일텐데 함정에 빠져서는 안되니 말이다.

**순환이 크롤러에게 해로운 이유**
- 루프에 빠져 꼼짝 못하게 만들 수 있다.
- 같은 페이지를 반복해서 가져오면 웹 서버에게 부담이 된다.
- 크롤러를 사용하는 애플리케이션은 중복 컨텐츠가 넘치게된다.

### 방문한 URL 기록
웹에 존재하는 수많은 `URL`에 대해 방문 기록을 남기는 것은 매우 무거운 작업일 것이다.

따라서, 효과적인 자료구조를 사용해야한다.
> 딱 여기까지 읽었을 때 Hash Table + Linked List가 생각이났다.
> 
> Hash Table은 검색에 매우 빠르고 충돌의 문제는 한 버킷에 Linked List를 이어붙이는 식으로 해결할 수 있을테니까?


책에서 설명하는 대표적인 기법 몇가지는 아래와 같다.
- 트리와 해시테이블
- 느슨한 존재 비트맵
- 체크포인트
- 파티셔닝

### 별칭
약속된 규약(well-known port 같은)들에 의해 분명 다른 `URL`이지만 같은 곳을 향하고 있을 수도 있다.
- well-known port
- 이스케이프 문자
- 대소문자 구분여부 등등

### URL 정규화 
1. 포트가 명시되지 않았다면 `well-known` 포트를 호스트 명에 추가
2. 이스케이핑된 문자를 대응되는 문자로 변한
3. `#`태그를 제거

서버가 대소문자를 구분하는지 가상 호스팅을 하는지등 추가적인 정보가 완벽히 없는 상태에서 모든 중복을 피할 수는 없다.

### 파일 시스템 링크 순환
**그림 9-3(b)**와 같이 루프로 빠질 수 있는 링크 순환을 발견하지 못하면 문제가 발생

### 동적 가상 웹 공간
악의적인 사이트가 웹 로봇들을 함정에 빠뜨리기 위해 `HTML`내부에 끊임없이 링크를 참조시켜 무한정 호출하게 만들 수 있다.

### 루프, 중복 피하기
아쉽게도 모든 함정을 피하는 방법은 없다.

책에서 언급한 것과 같이 **Trade-Off**가 발생한다.

**로봇이 더 잘 동작하기 위한 기법**
- URL 정규화
  - 앞서 살펴본 것과 같이 URL을 특정 규칙을 통해 변환해 표준 형태로 변환
- 너비 우선 크롤링
  - 크롤러를 재귀적으로 호출하도록 하지 않고 너비 우선으로 스케줄링하면 순환의 영향을 최소화할 수 있음
  - DFS와 BFS를 떠올려보자
  - ![image](https://user-images.githubusercontent.com/60773356/147851663-2d8a6ead-1bfc-4566-a9d4-3475f2c5ee0a.png)
- 스로틀링
  - 일정 시간 동안 가져올 수 있는 페이지의 숫자를 제한
- URL 크기 제한
  - 일정 길이를 넘는 크롤링을 거부
  - 순환으로 인해 URL이 계속 길어지는 경우를 방지(파일 시스템 링크 순환과 연관지어 생각)
  - 악의적인 URL이 아닌 진짜 URL이 긴 경우에 가져오지 못하는 경우가 발생할 수 있다는 문제는 존재
  - 이를 해결하기 위해 거부하는 것이 아닌 에러 로그를 남겨 모니터링하는 사용자에게 위험 신호를 제공할 수 있음
- URL/사이트 블랙리스트
  - 악의적인 사이트가 발견될 때마다 목록에 추가하고 관리(테이블처럼)
- 패턴 발견
  - 반복적인 구성요소를 가진 URL의 크롤링을 거부
- 컨텐츠 지문
  - 페이지의 컨텐츠를 체크섬으로 검사
  - 해싱처럼 특정 함수를 통과시켜 결과값 추출 ==> 만약 같은 체크섬이 나온다면 이미 본 것
- 사람의 모니터링

<br>

------------------------

<br>

## 9.2 로봇의 HTTP
로봇또한 `HTTP` 명세 규칙을 지켜야한다.

`HTTP Request`를 만들고 적절한 헤더를 사용해야한다.

### 요청 헤더 식별
`HTTP Header`중 몇 가지 헤더를 통해 서버에게 로봇의 정보를 전달해주어야한다.

`User-Agent`는 서버와 네트워크 피어가 요청하는 응용 프로그램, 운영 체제, 공급 업체, 버전을 식별하게 도와주는 헤더이다.

`From` 요청한 사용자 에이전트를 제어하는 인간 사용자의 인터넷 이메일을 포함하는 헤더이다.

`Accept`는 서버에게 어떤 미디어 타입을 받을 수 있는지 알려주는 헤더이다.

`Referer` 현재 요청을 보낸 페이지의 절대 혹은 부분 주소를 포함하는 헤더로써 유입경로를 서버에게 알려줄 수 있는 헤더이다.

### 가상 호스팅
하나의 서버가 여러 사이트를 호스팅하고 있을 때 호스트명을 명확히 전하지 않으면 원치않는 응답이 반환될 수 있다.

### 조건부 요청
자신이 이전에 요청한 시간과 다른 것이 있는지 알아보는 요청이다.

`if-modified-since`와 비슷한 것 같다.

### 응답 다루기
대부분의 로봇은 `GET` 메서드로 콘텐츠를 조회하지만 서버와 상호작용을 원하는 로봇은 `HTTP Response`를 다룰 줄 알아야한다.
- 상태코드
  - 상태코드를 이해할 줄 알아야함
- 엔터티
  - `Http Message`, 본문을 보고 그 의미를 이해할 줄 알아야함

### User-Agent 타겟팅
서버는 많은 로봇이 방문할 것임을 예상하고 이를 대응할 수 있어야한다.

<br>

---------------------------

<br>

## 9.3 부적절하게 동작하는 로봇들
부적절하게 동작하는 로봇들은 서버에 영향을 줄 수 있다!
- 폭주하는 로봇
  - 흔히 빠른 네트워크 위의 빠른 컴퓨터 위에서 동작하므로 서버에 부하를 줄 수 있기 때문에 로봇 사용자들은 보호 장치를 마련해야함
- 오래된 URL
  - 오래된 URL에 대해 빠른 로봇들이 서버에 계속 요청을 보내면?
  - 에러 페이지를 응답
  - 응답을 하는 자체가 서버의 자원을 사용하는 것이므로 문제가 된다.
- 길고 잘못된 URL
- 호기심이 지나친 로봇
  - 보안적인 이슈로써 개인의 민감 정보가 로봇에 의해 크롤링 될 수 있기 때문에 이를 무시하는 것도 중요
- 동적 게이트웨이 접근
  - 서버에서 복잡하게 연결된 API를 타는 요청을 로봇들이 하는 것도 서버에게 부담

<br>

----------------------------------

<br>

## 9.4 로봇 차단하기
로봇의 접근을 제어하는 정보를 저장하는 파일이 `robots.txt`이다.

루트에 `robots.txt` 파일을 제공해 로봇이 어떤 파일에 접근할 수 있고 어디에 접근해선 안되는지를 알려줄 수 있다.

![image](https://user-images.githubusercontent.com/60773356/147851671-d6a685dc-5657-4aae-8c81-0131803704bf.png)
- 내 블로그의 `robots.txt`
- Crawl-delay : 재방문 시간제한 

로봇들은 항상 서버의 `robots.txt`를 확인한 후에 크롤링을 진행해야한다.

### 로봇 차단 표준
이는 1994년 6월에 처음으로 만들어졌으며, 권고안이라고한다.

로봇이 `robots.txt` 파일을 읽고 접근을 중지하는 것을 목적으로 한다.

### 웹 사이트와 robots.txt 파일들
웹 사이트에 방문하기 전에 `robots.txt`파일이 존재한다면 로봇을 반드시 그 파일을 가져와 처리해야한다.

존재하지않는다면 해당 서버는 로봇의 모든 접근을 허용하는 것으로 간주한다.

- `robots.txt` 가져오기(GET 메서드 사용)
- 응답코드 읽기
  - `200` : `robots.txt` 파싱
  - `404` : 서버가 모든 접근을 허용한다고 간주
  - `401`, `403` : 서버가 로봇의 모든 접근을 제한했다고 간주
  - `503` : 해당 사이트에 대한 접근을 보류
  - `300번대` : 리다이렉트를 계속 따라가야함

### robots.txt 포맷
빈줄을 기점으로 하나의 레코드이다.

![image](https://user-images.githubusercontent.com/60773356/147851671-d6a685dc-5657-4aae-8c81-0131803704bf.png)
- 여기서는 3개의 레코드가 존재하는 것

`User-Agent: minchul`과 같이 `필드: 값`의 형태로 각 줄은 구성된다.

- `User-Agent`
  - 각 레코드는 하나 이상의 `User-Agent`줄로 시작한다.(*은 모두 허용을 의미)
  - 로봇 이름을 대소문자를 구분하지 않는 부분 문자열과 비교하므로 의도하지 않게 일치하는 경우를 주의
- `Disallow`, `Allow`
  - 접두 매칭을 허용하므로 주의해야한다.
  - 이스케이핑된 문자도 주의해야한다.

### 추가로 알아둘 점
- `User-Agent`, `Disallow`, `Allow` 외에 다른 필드를 포함할 수도 있는데, 로봇 자신이 이해하지 못한다면 무시해야함
- 한 줄을 여러 줄로 나누어 적는 것은 허용되지 않음
- 주석은 파일의 어디에서든 허용된다.(`#` 사용)
  - `# 이거슨 주석`
- 표준버전 0.0은 `Allow`를 지원하지 않음

### robots.txt 캐싱과 만료
`robots.txt`도 캐싱이 가능하므로 로봇은 주기적으로 해당 서버의 `robots.txt`를 캐싱해야한다.

당연하게도 만료에도 신경을 써야함!

### HTML 로봇 제어 META 태그
`robots.txt`의 단점 중 하나는 웹 사이트 관리자가 소유한다는 것이다.(컨텐츠 소유자 개개인이 아닌)

이를 위해 `HTML` 태그를 통해 직접 로봇 제어 태그를 추가할 수 있다.(명령들을 충돌나게 해선 안된다.)
- **NOINDEX** : 이 페이지를 처리하지말고 무시해
- **NOFOLLOW** : 이 페이지에 링크된 페이지를 크롤링하지마
- **INDEX** : 이 페이지를 인덱싱해도 괜찮아
- **FOLLOW** : 이 페이지에 링크된 페이지를 크롤링해도돼
- **NOARCHIVE** : 이 페이지를 캐싱하지마
- **ALL** : INDEX + FOLLOW
- **NONE** : NOINDEX + NOFOLLOW

이는 `HTML`의 `HEAD`에 추가되어야하며 값은 대소문자를 구분하지 않는다.

**추가적인 태그**
- **DESCRIPTION**
- **KEYWORDS**
- **REVISIT-AFTER**

<br>

---------------------------------

<br>

## 9.5 로봇 에티켓
[링크](http://www.robotstxt.org/)

위 링크에 방문하면 `robots.txt`에 대한 정보들을 얻을 수 있다.

<br>

------------------------------------

<br>

## 9.6 검색엔진
웹 로봇을 가장 광범위하게 사용하는 것은 검색 엔진이다.

이를 통해 사용자가 원하는 문서를 찾을 수 있다.

사용자가 어떤 문서를 검색하면 검색엔진 내의 색인 DB에서 문서를 찾는다.

웹 로봇은 웹 상에 존재하는 수 많은 문서를 긁어와 이 색인 DB에 저장한다.
- 변할 수 있기 때문에 이미 방문한 사이트도 주기적으로 다시 방문한다고 한다.

### 검색 엔진의 종류
- **로봇 검색 엔진**
  - 크롤러라고 불리는 로봇을 이용하여 웹상의 데이터를 효율적으로 수집하고, 이렇게 수집한 데이터를 키워드(keyword) 색인을 통해 사용자에게 제공하는 검색 엔진 
  - `Google`, `Naver` 등 현재 사용되는 대부분의 검색 엔진이 이 방식을 채택
  - 책에서 설명하는 `full-text-index`가 이런 방식으로 동작하는 것 같음

- **디렉토리 검색 엔진** 
  - 주제 분류에 의한 검색(디렉토리 서비스)을 제공하는 검색 엔진이며, 데이터의 분류를 사람이 직접 수행 
  - 현재 주류인 방식은 아니며, 1990년대 Yahoo 등에서 사용
  
- **메타 검색 엔진** 
  - 자체적으로는 정보를 보유하고 있지 않으면서 사용자가 입력한 키워드를 복수의 다른 검색 엔진으로 전송하여 결과를 얻고, 그 결과들을 종합하여 표시만 해 주는 검색 엔진 
  - 여러 검색 엔진의 결과를 동시에 보여주기 때문에 결과를 한눈에 살펴보기에는 편하지만, 메타 검색이라는 과정을 한 번 더 거쳐야 하므로 속도가 느릴 수 있음

### 대략적인 동작 과정
1. 사용자가 질의를 웹 검색엔진 게이트웨이로 전송
2. 질의에 해당하는 문서를 색인하여 응답

### 관련도 랭킹과 최적화
검색 엔진의 가장 큰 경쟁력은 바로 `SEO`(Search Engine Optimization) 검색 엔진 최적화이다.

색인된 수많은 사이트들 중에서 관련도가 높은 순서대로 정렬하여 사용자에게 보여주는 것을 의미한다.

![image](https://user-images.githubusercontent.com/60773356/147851675-4daeb568-e4b6-4d8c-a390-4c6d3d6fe1c6.png)
- 구글에 우리가 특정 단어를 입력했을 때 가장 관련도가 높은 순서대로 정렬하여 나타냄

### 스푸핑
사이트의 주인장들은 모두 자신의 사이트가 검색 엔진의 상단에 위치하기를 원한다.

이를 목적으로 알고리즘을 속이기위한 행위를 **스푸핑**이라고한다. 
> 보안쪽 용어로는 자신의 `IP`주소를 다른 사람의 `IP`주소로 속여 시스템에 접근하는 것을 **스푸핑**이라고한다.

